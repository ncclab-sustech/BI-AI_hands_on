{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4981038e-6c84-4135-8bd7-e9951ee239d6",
   "metadata": {},
   "source": [
    "# üèûÔ∏è Part 2: Data Exploration\n",
    "With the basics out of the way, we can now jump into image recognition. Before we begin, we are going to load some useful Python libraries into our environment. You don't need to have a precise understanding of what each library does, but here is a rough summary.\n",
    "\n",
    "| Library | Description |\n",
    "| --- | --- |\n",
    "| `torch` | This is the core `PyTorch` library doing most of the heavy lifting |\n",
    "| `torchvision` | This is how we will extract the data (if needed), and apply some cleaning to images |\n",
    "| `random` | This library provides random number generators. This is important if we want to sample images from our dataset | \n",
    "| `matplotlib.pyplot` | This is a very popular plotting library |\n",
    "| `time` | This has functions which allows us to keep track of how long the code takes to run |\n",
    "| `IPython` | This libraries allows us to access some of the options of these notebooks |\n",
    "| `numpy` | This is a very popular data manipulation library |\n",
    "| `helper` | A user-defined module (see the `./helper` subfolder for more details) of useful functions and classes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33c148-5646-45a1-91e0-73bde9be79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import torch \n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from helper import helper\n",
    "\n",
    "random.seed(2021) # We set a seed to ensure our samples will be the same every time we run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c2a17-3531-463b-89d9-b6cf3a035790",
   "metadata": {},
   "source": [
    "## ‚öóÔ∏è The Data Science Pipleine \n",
    "*This section will be repeated in both Part 2 and Part 3*\n",
    "\n",
    "> What on earth is data science?! -- George Washington (probably not)\n",
    "\n",
    "Seriously though, nowadays, in such a data-rich world, data science has become the new buzzword, the new cool kid in the block. But what exactly is it? Unfortunately, no one can really pin down a [rigourous definition](https://hdsr.mitpress.mit.edu/pub/jhy4g6eg/release/7) of data science. At the high level:\n",
    "\n",
    "> Data science is the systematic extraction of novel insight from data.\n",
    "\n",
    "Good enough! With this definition, most practitioners can somewhat agree on a pipeline or flow. Here are the steps:\n",
    "1. Identify your problem (What are you trying to do?)\n",
    "2. Obtain your data (What resource do we have to work with?)\n",
    "3. Explore your data (What does our data actually look like?)\n",
    "4. Prepare your data (How do we clean/wrangle our data to make it ingestible?)\n",
    "5. Model your data (How do we automate the process of drawing out insights?)\n",
    "6. Evaluate your model (How good are our predictions?)\n",
    "7. Deploy your model (How can the wider-user base access these insights?)\n",
    "\n",
    "The 7th step is out-of-scope for this workshop, but we well be exploring the other steps to varying degrees:\n",
    "* Steps 1-4 will be explored in Part 2.\n",
    "* Steps 5-6 will be explored in Part 3 and Part 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5ba8e-6ae6-47b3-bbf5-8fb23a060fae",
   "metadata": {},
   "source": [
    "## Step 1: Identify Your Problem \n",
    "![cc](../images/confused_cat.jpg)\n",
    "\n",
    "**Figure:** A day-to-day snapshot of a data scientist at work. ([source](https://s.keepmeme.com/files/en_posts/20200925/confused-cat-looking-at-computer-with-a-lot-of-question-marks-meme-861f3efff59aedea603e35b8c3c059f0.jpg))\n",
    "\n",
    "### The problem:\n",
    "* Your boss comes up to you and gives you a stack of unlabelled black & white photos\n",
    "* You get told you need to identify what item of clothing each photo represents (eg. t-shirt)\n",
    "* You get a stack of 70,000 labelled picture to give you an idea of the task\n",
    "\n",
    "*What do you do?!* Sure, you can label them by hand if there are only 100 or 1000 unlabelled images. But what if there are 1,000,000? This manual labelling is not tenable in the long term.\n",
    "\n",
    "Why use machine learning to automated image recognition?\n",
    "* **Scalable** -- provided you have a reasonable model and enough computational resources, getting a computer to label images is ***a lot*** easier.\n",
    "* **Consistent** -- the output of the model is going to be more consistent than any crack team of labelers you can assemble (we are but humans after all)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dac7d9-0306-4f4a-840b-70872631c283",
   "metadata": {},
   "source": [
    "## Step 2: Obtain Your Data\n",
    "Alright, you decided you probably won't manually label \\* phew \\*. What next? Lucky for you, your boss provided you with some initial information:\n",
    "* There are 70,000 labelled images.\n",
    "* Each image is an item of clothing.\n",
    "* Each image is a 28x28 sized image (784 pixels in total).\n",
    "* Each pixel is black and white, and has a value between 0 and 255 indicating the brightness of the pixel\n",
    "* There are a total of 10 types of items of clothing.\n",
    "\n",
    "| Label | Type of Clothing |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat | \n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle Boot |\n",
    "\n",
    "For example, the following is an image of a boot:\n",
    "\n",
    "![Boot](../images/boot.png)\n",
    "\n",
    "In fact, the dataset we are working with is called the [Fashion MNIST](https://www.kaggle.com/zalando-research/fashionmnist). In the cell below, we will try to extract the data and split them into the train and test sets (`train_iter` and `test_iter` respectively).\n",
    "\n",
    "***Note:*** These are black and white photos, but they are rendered with a greyscale palette because I forgot to change it at the start and now it's too late. That's a window into the problem of legacy code my friends! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39b273-db1c-4770-845e-174f641c2f0d",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "\n",
    "Why do we need to split the labelled data into train/test sets? (Don't worry, we will go through this in Step 4).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f64ea0-40e3-4bcc-817f-d682188761c5",
   "metadata": {},
   "source": [
    "---\n",
    "We will discuss the concept of **batch size** a bit more in Part 4. For now, let's just take the default values.\n",
    "\n",
    "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è If you clicked ![Binder](https://mybinder.org/badge_logo.svg) to get into the notebook, safely ignore the following. \n",
    "\n",
    "If you have opted to use your own anaconda to run this, we recommend the following parameters:\n",
    "* `batch_size = 256`\n",
    "* `n_workers = 4`\n",
    "\n",
    "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d2fcc-7942-4df7-9461-dcedec864de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the function without running it\n",
    "def load_data_fashion_mnist(batch_size, n_workers):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                    train=True,\n",
    "                                                    transform=trans,\n",
    "                                                    download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                   train=False,\n",
    "                                                   transform=trans,\n",
    "                                                   download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=n_workers),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=n_workers))\n",
    "\n",
    "# Then execute the function here\n",
    "batch_size = 512  # Set to 256 on your own device\n",
    "n_workers = 0      # Set to 4 on your own device\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size, n_workers = n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdacd7c5-f225-47bf-a2ef-800b8467585c",
   "metadata": {},
   "source": [
    "(Sometimes you may get an warning when running this cell. This can be safely ignored).\n",
    "\n",
    "#### üéâüéâüéâ Congratulations! You have just read in your train and test data! üéâüéâüéâ\n",
    "\n",
    "The format of `train_iter` and `test_iter` is a bit strange (they are what's called an iterable object -- we will discuss this in detail in the appendix if you are interested), so we have written a function to extract a single example. There is no need to fully understand each step, but there are comments if you wish to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75593710-a031-4557-b938-a7a444c53c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_sample(data):\n",
    "    \"\"\" Extract a single example from train_iter or test_iter\"\"\"\n",
    "    \n",
    "    # First we extract a few random batches (say 3), each of which will have up to your set batch_size (eg. 1024 for mybinder)\n",
    "    sampled_batches = random.sample(list(data), 3)\n",
    "    print(\"The number of mini-batches we have extracted are:\", len(sampled_batches))\n",
    "\n",
    "    # Second we select a single batch to look at, let's say the 3rd one\n",
    "    batch_no = 2\n",
    "    ## 0 denotes the predictors\n",
    "    ## 1 denotes the labels\n",
    "    predictor = sampled_batches[batch_no][0]\n",
    "    label = sampled_batches[batch_no][1]\n",
    "    print(\"Out of the\", len(sampled_batches), \"mini-batches, we have selected the\", batch_no + 1, \"th one.\")\n",
    "    print(\"The number of images in the mini-batch we selected are:\" , len(predictor), \" and \", len(label), \". Note these two values should be equal.\")\n",
    "\n",
    "    # Third, we select a single example in the batch, let's say the 100th one\n",
    "    example_no = 99\n",
    "    single_predictor = predictor[example_no]\n",
    "    single_label = label[example_no]\n",
    "    return (single_predictor, single_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e3e7bc-cf6e-4a93-8a14-dd269bb1187e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "single_predictor, single_label = get_single_sample(train_iter)\n",
    "\n",
    "print(\"The shape of the predictor\", single_predictor.shape)\n",
    "print(\"The shape of the label\", single_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ce384-34a9-491f-8ff4-8b8a4eb7a0f5",
   "metadata": {},
   "source": [
    "The `shape()` function shows us the **dimensions** of our arrays. From the single example we have extracted:\n",
    "* The predictor has `[1, 28, 28]`: this is a tensor\n",
    "    * The `1` represents the number of channels (since it's black and white this is only =1. For colour, it =3 since it's RGB).\n",
    "    * The `28` and `28` represents the dimension of the image (28x28)\n",
    "* The label has `[]`: this is a scalar and represents the type of clothing (the target label we are trying to predict)\n",
    "\n",
    "***Note:*** Dimensions puts a kinda *limit* on the number of indices. Consider the `[1,28,28]` example. Since there's only one colour, you can only ever use the index `0` for the first dimension. Likewise you can go from `0` to `27` with the 2nd and 3rd dimensions.\n",
    "\n",
    "Now let's try to visual it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbdce18-f4a0-49d6-9700-edb2efd592ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(single_predictor[0]) # print only the one channel of BW\n",
    "plt.show()\n",
    "\n",
    "print(\"This shows an image of: \", int(single_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acb39c-fe12-4b4b-96ee-00eeacb7edd6",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "Does the image match the label? (See table above).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f5824-cc60-4579-a26e-dc6c58500e6a",
   "metadata": {},
   "source": [
    "You may get the following error if you run the above cell to show the image:\n",
    "\n",
    "```\n",
    "\"OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\n",
    "OMP: Hint: This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\"\n",
    "```\n",
    "\n",
    "If so, this may be a fix. Uncomment the cell below and run it. Caution: There may be [side-effects](https://github.com/dmlc/xgboost/issues/1715) to this fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa4a808-81f6-40bb-9979-ed95bc6abcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b468ca6-1978-4525-8b2d-3a401deb2212",
   "metadata": {},
   "source": [
    "## Step 3: Explore Your Data\n",
    "Often, you are not the one who has collected the data. Before doing any fancy modelling, the most important thing is to *understand* our data. Here are some questions we can ask ourselves:\n",
    "* What is the format of the predictors we have?\n",
    "* What is the format of the predictors we need for our model?\n",
    "* How many output classes do we have? Or in the case of regression, what is the distribution of the output label?  \n",
    "etc.\n",
    "\n",
    "Here we will focus on a single question: What is the **class distribution** of the train/test sets?\n",
    "* The class distribution is just examples of each class do we have -- for example are there more boots than there are t-shirts?\n",
    "* The class distribution in the train and test set should ideally be **similar** to ensure we are training a representative subset of the data\n",
    "* The class distribution in the labelled data should ideally be **similar** to any unseen examples (for example, if there are no boots in the training data, and we encounter a boot for the first time in unseen data, the model will perform very poorly on boots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f070b3f-a24b-4d1d-a20d-5dc8c9951fcf",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "Why do we prefer the train and test set to have similar class distributions?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1885bcec-2eaf-4e4d-9dfa-6132033f635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_explore(data_iter):\n",
    "    class_count = helper.Accumulator(10)\n",
    "    for i, (X, y) in enumerate(data_iter):\n",
    "        current_counter = torch.bincount(y)\n",
    "        class_count.add(current_counter[0], current_counter[1], current_counter[2], current_counter[3], current_counter[4],\n",
    "                  current_counter[5], current_counter[6], current_counter[7], current_counter[8], current_counter[9]) # This is bad coding practice, don't do this, I just got lazy!\n",
    "    for i in range(10):\n",
    "        print(\"Class\", i, \"has\", int(class_count.__getitem__(i)), \"images\")\n",
    "    # Note sometimes we can omit the return clause: https://stackoverflow.com/questions/13307158/most-pythonic-way-of-function-with-no-return\n",
    "        \n",
    "print(\"Train Data:\")\n",
    "data_explore(train_iter)\n",
    "print(\"Test Data\")\n",
    "data_explore(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d3a7c-b818-48ac-ab9d-c352340d9d38",
   "metadata": {},
   "source": [
    "![](../images/thanos.jpg)  \n",
    "([source](https://i.kym-cdn.com/entries/icons/original/000/027/257/perfectly-balanced-as-all-things-should-be.jpg))\n",
    "\n",
    "In reality, your data won't be this perfect, it's always worth checking and understanding class balances!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821303b4-092e-45cd-9b32-6448122e0166",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Your Data\n",
    "This was something we have previously alluded to, but we have 70,000 labelled examples. Perfect! Do we throw them all into the training/fitting process of the model?\n",
    "\n",
    "The answer is **NO**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c052289-dcb5-432d-9053-9646d9e1456f",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "Why is that though? Doesn't more data = better model?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba420027-5351-4c8b-a3fa-18623ce53076",
   "metadata": {},
   "source": [
    "![yesbutno](../images/yesbutno.jpg)\n",
    "\n",
    "The reason lies in a concept known as **overfitting**. Remember, our goal is to make sure the model works well on hitherto unseen data (ie. unlabelled data). If we just throw all the data we have into the train process, then we won't have access to an independent dataset to evaluate the model. Later on, we will see how the test data can be used to assess overfitting. For now, just remember that it is vital that we have an (representative) subset of the labelled data set aside for evaluation. For the purposes of this example, 60,000 images will be used to train and 10,000 images will be used to test:\n",
    "* `Train` -- this is the data we use to fit the model (n=60,000)\n",
    "* `Validate` -- we will not be using this today to tune hyper-parameters\n",
    "* `Test` -- this is the data we use to determine the model performance (n=10,000)\n",
    "\n",
    "**Hyperparameter** = parameters the user (you!) choose beforehand (as opposed to parameters the model learns from the data, such as the weights of a neural network) \n",
    "\n",
    "e.g. the batch number, the number of layers in a neural network, the width of each layer etc.\n",
    "\n",
    "*How do you choose?*  \n",
    "The *art* of choosing the correct hyperparameters is called **hyperparameter tuning** and a detailed discussion is beyond the scope of this workshop.\n",
    "\n",
    "The most simple implementation of tuning is **grid search**, and it's predicated on the idea that you have a limited list of hyperparameters to *try*. For example, let's say we are deciding how many layers to have in our neural network. We can train on a few 'settings' (eg. `n_hidden_layers = {1,2,3,4,5}`). By evaluating these models against the `Validate` set, we can find the hyperparameter which gives the optimal result.\n",
    "\n",
    "| `n_hidden_layers` | Validate Accuracy |\n",
    "| --- | --- |\n",
    "| 1 | 0.645 | \n",
    "| 2 | 0.823 | \n",
    "| 3 | 0.855 | \n",
    "| 4 | 0.899 | \n",
    "| 5 | 0.878 |\n",
    "\n",
    "If we get these results, we choose `n_hidden_layers = 4`. In reality we might have many of these parameters which make up a 'grid', and we 'search' through this parameter space until we find one with the best accuracy on the validate set.\n",
    "\n",
    "***Note:*** The validate set has to be separate from the test set as overzealous parameter tuning can overfit the model. So if we had used the test set to tune hyperparameters, we may never truly know if our model has overfit (until our boss comes back to us with all the bad performance reports!). The test set should ONLY every be used for evaluating the final accuracy of the model and no more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc21d3-9baa-4185-9c2d-a97df9924c8d",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=Red> **[BONUS] Discussion:** </font> \n",
    "What are some shortcomings of grid search?\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-tutorial",
   "language": "python",
   "name": "cnn-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
