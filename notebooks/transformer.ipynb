{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/self-attention1.png\" alt=\"perplexity\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Implement Self-Attention for a Single Head\n",
    "\n",
    "First, prepare the input data in shape $N\\times d$\n",
    "\n",
    "*Hint*: \n",
    "- Use `torch.randn` to generate a torch tensor in the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "d = 512\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### START YOUR CODE ###\n",
    "X = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test \n",
    "assert isinstance(X, torch.Tensor)\n",
    "print('X.size():', X.size())\n",
    "print('X[:,0]:', X[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# X.shape: (3, 512)\n",
    "# X[:,0]: [-1.1258398  -0.54607284 -1.0840825 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/self-attention.png\" alt=\"perplexity\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, initialize weight matrices $W^Q$, $W^K$, and $W^V$. We assume they are for a single head, so $d_k=d_v=d$\n",
    "\n",
    "Using $W^Q$ as an example\n",
    "- First initialize an empty tensor `W_q` in the dimension of $d\\times d_k$, using the `torch.empty()` function. Then initialize it with `nn.init.xavier_uniform_()`.\n",
    "- After `W_q` is initialized, obtain the query matrix `Q` with a multiplication between `X` and `W_q`, using `torch.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Do not remove this line\n",
    "\n",
    "n_heads = 1\n",
    "\n",
    "### START YOUR CODE ###\n",
    "d_k = d // n_heads # Compute d_k\n",
    "\n",
    "W_q = None\n",
    "W_k = None\n",
    "W_v = None\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q = None\n",
    "K = None\n",
    "V = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "assert Q.size() == (N, d_k)\n",
    "assert K.size() == (N, d_k)\n",
    "assert V.size() == (N, d_k)\n",
    "\n",
    "print('Q.size():', Q.size())\n",
    "print('Q[:,0]:', Q[:,0].data.numpy())\n",
    "print('K.size():', K.size())\n",
    "print('K[:,0]:', K[:,0].data.numpy())\n",
    "print('V.size():', V.size())\n",
    "print('V[:,0]:', V[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# Q.size(): torch.Size([3, 512])\n",
    "# Q[:,0]: [-0.45352045 -0.40904033  0.18985942]\n",
    "# K.size(): torch.Size([3, 512])\n",
    "# K[:,0]: [ 1.509987   -0.5503683   0.44788954]\n",
    "# V.size(): torch.Size([3, 512])\n",
    "# V[:,0]: [ 0.43034226  0.00162293 -0.1317436 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, compute the attention scores $\\alpha$ and the weighted output\n",
    "\n",
    "Following the equation:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "*Hint*:\n",
    "- $\\alpha = \\text{softmax}(\\frac{QK^\\top}{\\sqrt{d_k}})$, where you can use `torch.nn.functional.softmax()` to compute the softmax. Pay attention to the `dim` parameter.\n",
    "- The weighted output is the multiplication between $\\alpha$ and $V$. Pay attention to their dimensions: $\\alpha$ is of shape $N\\times N$, and $\\alpha_{ij}$ is the attention score from the $i$-th to the $j$-th word. \n",
    "- The weighted output is of shape $N\\times d_v$, and here we assume $d_k=d_v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "alpha = None\n",
    "output = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "assert alpha.size() == (N, N)\n",
    "assert output.size() == (N, d_k)\n",
    "\n",
    "print('alpha.size():', alpha.size())\n",
    "print('alpha:', alpha.data.numpy())\n",
    "print('output.size():', output.size())\n",
    "print('output[:,0]:', output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# alpha.size(): torch.Size([3, 3])\n",
    "# alpha: [[0.78344566 0.14102352 0.07553086]\n",
    "#  [0.25583813 0.18030964 0.5638523 ]\n",
    "#  [0.09271843 0.2767209  0.63056064]]\n",
    "# output.size(): torch.Size([3, 512])\n",
    "# output[:,0]: [ 0.32742795  0.03610666 -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Mask Future Tokens\n",
    "\n",
    "First, create a binary mask tensor of size $N\\times N$, which is lower triangular, with the diagonal and upper triangle set to 0.\n",
    "\n",
    "*Hint*: Use `torch.tril` and `torch.ones`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mask.png\" alt=\"perplexity\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "mask = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('mask:', mask.data.numpy())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# mask: [[1. 0. 0.]\n",
    "#  [1. 1. 0.]\n",
    "#  [1. 1. 1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the mask to fill the corresponding future cells in $QK^\\top$ with $-\\infty$ (`-np.inf`), and then pass it to softmax to compute the new attention scores.\n",
    "\n",
    "*Hint*: Use `torch.Tensor.masked_fill` function to selectively fill the upper triangle area of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "new_alpha = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "print('new_alpha:', new_alpha.data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# new_alpha: [[1.         0.         0.        ]\n",
    "#  [0.5865858  0.41341412 0.        ]\n",
    "#  [0.09271843 0.2767209  0.63056064]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the output should also be updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "new_output = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('new_output.size():', new_output.size())\n",
    "print('new_output[:,0]:', new_output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# new_output.size(): torch.Size([3, 512])\n",
    "# new_output[:,0]: [ 0.43034226  0.2531036  -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Integrate Multiple Heads\n",
    "\n",
    "Finally, integrate the above implemented functions into the `MultiHeadAttention` class.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- In this class, the weight matrices `W_q`, `W_k`, and `W_v` are defined as tensors of size $d\\times d$. Thus, the output $Q=XW^Q$ is of size $N\\times d$.\n",
    "\n",
    "- Then we reshape $Q$ (and $K$, $V$ as well) into the tensor `Q_` of shape $N\\times h\\times d_k$, where $h$ is the number of heads (`n_heads`) and $d_k = d // h$. Similar operations are applied to $K$ and $V$. \n",
    "\n",
    "- The multiplication $QK^\\top$ is now between two tensors of shape $N\\times h\\times d_k$, `Q_` and `K_`, and the output is of size $h\\times N \\times N$. Thus, you need to use `torch.matmul` and `torch.permute` properly to make the dimensions of `Q_`, `K_`, and `V_` be in the correct order.\n",
    "\n",
    "- Also, remember to apply the future mask to each attention head's output. You can use `torch.repeat` to replicate the mask for `n_heads` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_q = nn.Parameter(torch.empty(d_model, d_model))\n",
    "        self.W_k = nn.Parameter(torch.empty(d_model, d_model))\n",
    "        self.W_v = nn.Parameter(torch.empty(d_model, d_model))\n",
    "\n",
    "        nn.init.xavier_normal_(self.W_q)\n",
    "        nn.init.xavier_normal_(self.W_k)\n",
    "        nn.init.xavier_normal_(self.W_v)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        N = X.size(0)\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        Q = None\n",
    "        K = None\n",
    "        V = None\n",
    "        Q_ = None\n",
    "        K_ = None\n",
    "        V_ = None\n",
    "\n",
    "        # Raw attention scores\n",
    "        alpha = None\n",
    "        # Apply the mask\n",
    "        mask = None\n",
    "        alpha = None\n",
    "        # Softmax\n",
    "        alpha = None\n",
    "\n",
    "        output = None\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "torch.manual_seed(0)\n",
    "\n",
    "multi_head_attn = MultiHeadAttention(d, n_heads=1)\n",
    "output = multi_head_attn(X)\n",
    "\n",
    "assert output.size() == (N, d)\n",
    "print('output.size():', output.size())\n",
    "print('output[:,0]:', output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# output.size(): torch.Size([3, 512])\n",
    "# output[:,0]: [ 0.43034226  0.2531036  -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that the above output size and values should be the same as the previous one, as we used `n_heads=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/transformer.png\" alt=\"perplexity\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Self-attention with residual connection and layer norm\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "\n",
    "        # 2. Feed-forward network with residual connection and layer norm\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, num_layers, vocab_size, max_len):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(src)  # (N, seq_len, d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # Pass through each Transformer encoder layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Output layer (project to vocabulary size)\n",
    "        output = self.fc_out(x)  # (N, seq_len, vocab_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # Self-attention on the target sequence\n",
    "        tgt_attn = self.self_attention(tgt)\n",
    "        tgt = self.norm1(tgt + tgt_attn)\n",
    "\n",
    "        # Cross-attention between target and encoder memory\n",
    "        cross_attn = self.cross_attention(tgt, memory)\n",
    "        tgt = self.norm2(tgt + cross_attn)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ff_output = self.ff(tgt)\n",
    "        tgt = self.norm3(tgt + ff_output)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, num_layers, vocab_size, max_len):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # Embedding and positional encoding for target sequence\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "\n",
    "        # Pass through each Decoder layer\n",
    "        for layer in self.layers:\n",
    "            tgt_emb = layer(tgt_emb, memory)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc_out(tgt_emb)\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
