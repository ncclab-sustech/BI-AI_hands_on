{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/self-attention1.png\" alt=\"perplexity\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Implement Self-Attention for a Single Head\n",
    "\n",
    "First, prepare the input data in shape $N\\times d$\n",
    "\n",
    "*Hint*: \n",
    "- Use `torch.randn` to generate a torch tensor in the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.size(): torch.Size([3, 512])\n",
      "X[:,0]: [-1.1258398  -0.54607284 -1.0840825 ]\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "d = 512\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### START YOUR CODE ###\n",
    "X = torch.randn(N, d)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test \n",
    "assert isinstance(X, torch.Tensor)\n",
    "print('X.size():', X.size())\n",
    "print('X[:,0]:', X[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# X.shape: (3, 512)\n",
    "# X[:,0]: [-1.1258398  -0.54607284 -1.0840825 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/self-attention.png\" alt=\"perplexity\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, initialize weight matrices $W^Q$, $W^K$, and $W^V$. We assume they are for a single head, so $d_k=d_v=d$\n",
    "\n",
    "Using $W^Q$ as an example\n",
    "- First initialize an empty tensor `W_q` in the dimension of $d\\times d_k$, using the `torch.empty()` function. Then initialize it with `nn.init.xavier_normal_()`.\n",
    "- After `W_q` is initialized, obtain the query matrix `Q` with a multiplication between `X` and `W_q`, using `torch.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.size(): torch.Size([3, 512])\n",
      "Q[:,0]: [-0.45352072 -0.40904027  0.18985987]\n",
      "K.size(): torch.Size([3, 512])\n",
      "K[:,0]: [ 1.5099866  -0.550368    0.44788927]\n",
      "V.size(): torch.Size([3, 512])\n",
      "V[:,0]: [ 0.4303431   0.00162256 -0.13174425]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) # Do not remove this line\n",
    "\n",
    "n_heads = 1\n",
    "\n",
    "### START YOUR CODE ###\n",
    "d_k = d // n_heads # Compute d_k\n",
    "\n",
    "W_q = torch.empty(d, d_k)\n",
    "W_k = torch.empty(d, d_k)\n",
    "W_v = torch.empty(d, d_k)\n",
    "\n",
    "nn.init.xavier_normal_(W_q)\n",
    "nn.init.xavier_normal_(W_k)\n",
    "nn.init.xavier_normal_(W_v)\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q = torch.matmul(X, W_q)\n",
    "K = torch.matmul(X, W_k)\n",
    "V = torch.matmul(X, W_v)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "assert Q.size() == (N, d_k)\n",
    "assert K.size() == (N, d_k)\n",
    "assert V.size() == (N, d_k)\n",
    "\n",
    "print('Q.size():', Q.size())\n",
    "print('Q[:,0]:', Q[:,0].data.numpy())\n",
    "print('K.size():', K.size())\n",
    "print('K[:,0]:', K[:,0].data.numpy())\n",
    "print('V.size():', V.size())\n",
    "print('V[:,0]:', V[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# Q.size(): torch.Size([3, 512])\n",
    "# Q[:,0]: [-0.45352045 -0.40904033  0.18985942]\n",
    "# K.size(): torch.Size([3, 512])\n",
    "# K[:,0]: [ 1.509987   -0.5503683   0.44788954]\n",
    "# V.size(): torch.Size([3, 512])\n",
    "# V[:,0]: [ 0.43034226  0.00162293 -0.1317436 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, compute the attention scores $\\alpha$ and the weighted output\n",
    "\n",
    "Following the equation:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "*Hint*:\n",
    "- $\\alpha = \\text{softmax}(\\frac{QK^\\top}{\\sqrt{d_k}})$, where you can use `torch.nn.functional.softmax()` to compute the softmax. Pay attention to the `dim` parameter.\n",
    "- The weighted output is the multiplication between $\\alpha$ and $V$. Pay attention to their dimensions: $\\alpha$ is of shape $N\\times N$, and $\\alpha_{ij}$ is the attention score from the $i$-th to the $j$-th word. \n",
    "- The weighted output is of shape $N\\times d_v$, and here we assume $d_k=d_v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha.size(): torch.Size([3, 3])\n",
      "alpha: [[0.7834456  0.14102356 0.07553087]\n",
      " [0.2558382  0.18030982 0.563852  ]\n",
      " [0.0927186  0.2767208  0.6305606 ]]\n",
      "output.size(): torch.Size([3, 512])\n",
      "output[:,0]: [ 0.32742846  0.0361065  -0.04272293]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "scale = torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "attention_scores = torch.matmul(Q, K.T) / scale\n",
    "alpha = F.softmax(attention_scores, dim=-1)\n",
    "output = torch.matmul(alpha, V)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "assert alpha.size() == (N, N)\n",
    "assert output.size() == (N, d_k)\n",
    "\n",
    "print('alpha.size():', alpha.size())\n",
    "print('alpha:', alpha.data.numpy())\n",
    "print('output.size():', output.size())\n",
    "print('output[:,0]:', output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# alpha.size(): torch.Size([3, 3])\n",
    "# alpha: [[0.78344566 0.14102352 0.07553086]\n",
    "#  [0.25583813 0.18030964 0.5638523 ]\n",
    "#  [0.09271843 0.2767209  0.63056064]]\n",
    "# output.size(): torch.Size([3, 512])\n",
    "# output[:,0]: [ 0.32742795  0.03610666 -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Mask Future Tokens\n",
    "\n",
    "First, create a binary mask tensor of size $N\\times N$, which is lower triangular, with the diagonal and upper triangle set to 0.\n",
    "\n",
    "*Hint*: Use `torch.tril` and `torch.ones`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: [[1. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "mask =  torch.tril(torch.ones(N, N))\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('mask:', mask.data.numpy())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# mask: [[1. 0. 0.]\n",
    "#  [1. 1. 0.]\n",
    "#  [1. 1. 1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the mask to fill the corresponding future cells in $QK^\\top$ with $-\\infty$ (`-np.inf`), and then pass it to softmax to compute the new attention scores.\n",
    "\n",
    "*Hint*: Use `torch.Tensor.masked_fill` function to selectively fill the upper triangle area of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mask.png\" alt=\"perplexity\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_alpha: [[1.         0.         0.        ]\n",
      " [0.58658576 0.4134143  0.        ]\n",
      " [0.0927186  0.2767208  0.6305606 ]]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "masked_attention_scores = attention_scores.masked_fill(mask == 0, -np.inf)\n",
    "new_alpha = F.softmax(masked_attention_scores, dim=-1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "print('new_alpha:', new_alpha.data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# new_alpha: [[1.         0.         0.        ]\n",
    "#  [0.5865858  0.41341412 0.        ]\n",
    "#  [0.09271843 0.2767209  0.63056064]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the output should also be updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_output.size(): torch.Size([3, 512])\n",
      "new_output[:,0]: [ 0.4303431   0.2531039  -0.04272293]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "new_output = torch.matmul(new_alpha, V)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('new_output.size():', new_output.size())\n",
    "print('new_output[:,0]:', new_output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# new_output.size(): torch.Size([3, 512])\n",
    "# new_output[:,0]: [ 0.43034226  0.2531036  -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Integrate Multiple Heads\n",
    "\n",
    "Finally, integrate the above implemented functions into the `MultiHeadAttention` class.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- In this class, the weight matrices `W_q`, `W_k`, and `W_v` are defined as tensors of size $d\\times d$. Thus, the output $Q=XW^Q$ is of size $N\\times d$.\n",
    "\n",
    "- Then we reshape $Q$ (and $K$, $V$ as well) into the tensor `Q_` of shape $N\\times h\\times d_k$, where $h$ is the number of heads (`n_heads`) and $d_k = d // h$. Similar operations are applied to $K$ and $V$. \n",
    "\n",
    "- The multiplication $QK^\\top$ is now between two tensors of shape $N\\times h\\times d_k$, `Q_` and `K_`, and the output is of size $h\\times N \\times N$. Thus, you need to use `torch.matmul` and `torch.permute` properly to make the dimensions of `Q_`, `K_`, and `V_` be in the correct order.\n",
    "\n",
    "- Also, remember to apply the future mask to each attention head's output. You can use `torch.repeat` to replicate the mask for `n_heads` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_q = nn.Parameter(torch.empty(d_model, d_model))\n",
    "        self.W_k = nn.Parameter(torch.empty(d_model, d_model))\n",
    "        self.W_v = nn.Parameter(torch.empty(d_model, d_model))\n",
    "\n",
    "        nn.init.xavier_normal_(self.W_q)\n",
    "        nn.init.xavier_normal_(self.W_k)\n",
    "        nn.init.xavier_normal_(self.W_v)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        N = X.size(0)\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # 1. Compute Q, K, V\n",
    "        Q = torch.matmul(X, self.W_q)  # (N, d_model)\n",
    "        K = torch.matmul(X, self.W_k)  # (N, d_model)\n",
    "        V = torch.matmul(X, self.W_v)  # (N, d_model)\n",
    "\n",
    "        # 2. Reshape into multi-head format: (N, n_heads, d_k)\n",
    "        Q_ = Q.view(N, self.n_heads, self.d_k).permute(1, 0, 2)  # (n_heads, N, d_k)\n",
    "        K_ = K.view(N, self.n_heads, self.d_k).permute(1, 0, 2)  # (n_heads, N, d_k)\n",
    "        V_ = V.view(N, self.n_heads, self.d_k).permute(1, 0, 2)  # (n_heads, N, d_k)\n",
    "\n",
    "        # 3. Raw attention scores: QK^T / sqrt(d_k)\n",
    "        attention_scores = torch.matmul(Q_, K_.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # (n_heads, N, N)\n",
    "\n",
    "        # 4. Apply the mask (for each head)\n",
    "        mask = torch.tril(torch.ones(N, N))  # (N, N) lower triangular mask\n",
    "        mask = mask.unsqueeze(0).repeat(self.n_heads, 1, 1)  # Repeat for each head (n_heads, N, N)\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, -np.inf)\n",
    "\n",
    "        # 5. Softmax over the attention scores\n",
    "        alpha = F.softmax(attention_scores, dim=-1)  # (n_heads, N, N)\n",
    "\n",
    "        # 6. Multiply by V_ to get the weighted output\n",
    "        output = torch.matmul(alpha, V_)  # (n_heads, N, d_k)\n",
    "\n",
    "        # 7. Concatenate heads: (N, d_model)\n",
    "        output = output.permute(1, 0, 2).contiguous().view(N, -1)  # (N, d_model)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.size(): torch.Size([3, 512])\n",
      "output[:,0]: [ 0.4303431   0.2531039  -0.04272293]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "torch.manual_seed(0)\n",
    "\n",
    "multi_head_attn = MultiHeadAttention(d, n_heads=1)\n",
    "output = multi_head_attn(X)\n",
    "\n",
    "assert output.size() == (N, d)\n",
    "print('output.size():', output.size())\n",
    "print('output[:,0]:', output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# output.size(): torch.Size([3, 512])\n",
    "# output[:,0]: [ 0.43034226  0.2531036  -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that the above output size and values should be the same as the previous one, as we used `n_heads=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/transformer.png\" alt=\"perplexity\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Self-attention with residual connection and layer norm\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "\n",
    "        # 2. Feed-forward network with residual connection and layer norm\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, num_layers, vocab_size, max_len):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(src)  # (N, seq_len, d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # Pass through each Transformer encoder layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Output layer (project to vocabulary size)\n",
    "        output = self.fc_out(x)  # (N, seq_len, vocab_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # Self-attention on the target sequence\n",
    "        tgt_attn = self.self_attention(tgt)\n",
    "        tgt = self.norm1(tgt + tgt_attn)\n",
    "\n",
    "        # Cross-attention between target and encoder memory\n",
    "        cross_attn = self.cross_attention(tgt, memory)\n",
    "        tgt = self.norm2(tgt + cross_attn)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ff_output = self.ff(tgt)\n",
    "        tgt = self.norm3(tgt + ff_output)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, num_layers, vocab_size, max_len):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # Embedding and positional encoding for target sequence\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "\n",
    "        # Pass through each Decoder layer\n",
    "        for layer in self.layers:\n",
    "            tgt_emb = layer(tgt_emb, memory)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc_out(tgt_emb)\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
