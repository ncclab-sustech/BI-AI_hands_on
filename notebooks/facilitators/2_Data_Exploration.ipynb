{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76d979d-a590-4a3e-8d33-8d449f7663a7",
   "metadata": {},
   "source": [
    "# üèûÔ∏è Part 2: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39b273-db1c-4770-845e-174f641c2f0d",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "\n",
    "Why do we need to split the labelled data into train/test sets? (Don't worry, we will go through this in Step 4).\n",
    "\n",
    "<font color='Red'> We need an independent dataset to ensure the model generalises and is not overfitting. </font> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acb39c-fe12-4b4b-96ee-00eeacb7edd6",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "Does the image match the label? (See table above).\n",
    "\n",
    "<font color='Red'> It should...</font> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f070b3f-a24b-4d1d-a20d-5dc8c9951fcf",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "Why do we prefer the train and test set to have similar class distributions?\n",
    "\n",
    "<font color='Red'> It means the training data is representative of the data we want to apply our model to. It doesn't have to be perfect, a loose similarity in distribution is usually good enough. </font> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c052289-dcb5-432d-9053-9646d9e1456f",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "Why is that though? Doesn't more data = better model?\n",
    "\n",
    "<font color='Red'> See discussion in main notebook.</font> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc21d3-9baa-4185-9c2d-a97df9924c8d",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "What are some shortcomings of grid search?\n",
    "\n",
    "<font color='Red'> Grid search does not scale well to hyperparameters - usually the hyperparameter space you have to search through is HUGE. There are guiding principles and folk lore around what are good hyperparameters and which ones to test for, but grid searching is like a brute force method. It can often be automated if you have the resources to run it on servers or high-performance computing clusters. There is, however, innvoation in the method front of hyperparameter tuning such as [Bayesian methods](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f).\n",
    "</font> \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-tutorial",
   "language": "python",
   "name": "cnn-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
