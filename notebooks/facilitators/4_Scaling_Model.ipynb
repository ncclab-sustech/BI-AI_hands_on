{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73a9c12-b053-4227-9fce-3801289bf63c",
   "metadata": {},
   "source": [
    "# ðŸš€ Part 4: To Infinity and Beyond!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce92ad-8620-4f7c-87b1-53288d4ff43a",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "How did that compare with our previous model? How does that compare with random assignment? What can be done to improve the model?\n",
    "\n",
    "<font color='Red'> It should be better than previous and random assignment. To improve the model, run it over more epochs, do hyperparameter training etc. </font> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8eeca6-adc7-4d82-a757-ec6157919b3e",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "What is the optimal number of epochs from the graph above?\n",
    "\n",
    "<font color='Red'> Around 25-30 </font> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c85e9-9b1c-4f8e-ab7d-a19a905361de",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "In what way is CNN preferred over MLP? Why might MLP look better in the 5 epoch plot?\n",
    "\n",
    "<font color=Red>  CNN seems to plateau at a higher accuracy than MLP, so once trained it is more performant. MLP has a higher accuracy for smaller epochs because perhaps it has fewer weights to update (easier to reach the optimal model). </font>\n",
    "\n",
    "<font color=Red>  As a bonus question, try implementing a 15 node MLP with the sigmoid as the activation function: </font>\n",
    "\n",
    "The functions below need to be copied to the main notebook to be run.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fb7a8-7bb0-4d24-9f4c-63b76ffdac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(in_features = 28 * 28, out_features = 15), nn.Sigmoid(), \n",
    "                    nn.Linear(in_features = 15, out_features = 10))\n",
    "\n",
    "mlp_net.apply(init_weights) # nb: this takes in a function as an argument\n",
    "lr = 0.9 \n",
    "optimizer = torch.optim.SGD(mlp_net.parameters(), lr=lr) \n",
    "loss = nn.CrossEntropyLoss() \n",
    "\n",
    "num_epochs = 5 # Set this to 10 if you are using your own device\n",
    "epochs = np.arange(num_epochs) + 1\n",
    "\n",
    "train_accuracy, test_accuracy = train_network_scaled(net = mlp_net, num_epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d89520-c2ef-4a7e-810d-75aaf021baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot it\n",
    "plt.plot(epochs, train_accuracy, label = \"Training Accuracy\")\n",
    "plt.plot(epochs, test_accuracy, label = \"Testing Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"# Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfaa1b5-58cc-4e30-baef-b9f890a6109d",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-tutorial",
   "language": "python",
   "name": "cnn-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
