{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73a9c12-b053-4227-9fce-3801289bf63c",
   "metadata": {},
   "source": [
    "# üöÄ Part 4: To Infinity and Beyond!\n",
    "\n",
    "---\n",
    "## ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è Warning! ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "This part will take a while to run (10-15min to train the network). It is highly recommended that you 'Restart the Kernel and re-run the notebook' at the *very beginning* before reading through it. This can be done by pressing the ‚è© button, then clicking 'restart'.\n",
    "\n",
    "---\n",
    "\n",
    "We are going to re-run the key cells used to set up our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61881d5b-bfee-4ecf-90cc-76cdb38e7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import torch \n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from helper import helper\n",
    "\n",
    "random.seed(2021) # We set a seed to ensure our samples will be the same every time we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe030e-f8b3-4fba-9c31-5c50bbae6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the function without running it\n",
    "def load_data_fashion_mnist(batch_size, n_workers):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                    train=True,\n",
    "                                                    transform=trans,\n",
    "                                                    download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                   train=False,\n",
    "                                                   transform=trans,\n",
    "                                                   download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=n_workers),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=n_workers))\n",
    "\n",
    "# Then execute the function here\n",
    "batch_size = 512  # Set to 256 on your own device\n",
    "n_workers = 0      # Set to 4 on your own device\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size, n_workers = n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f43e6-0632-4f9f-af27-0b22f5f96ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise LeNet Architecture (ans)\n",
    "net = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size=5), nn.Sigmoid(),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "                    nn.Linear(in_features = 16 * 5 * 5, out_features = 120), nn.Sigmoid(),\n",
    "                    nn.Linear(in_features = 120, out_features = 84), nn.Sigmoid(), \n",
    "                    nn.Linear(in_features = 84, out_features = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d107d5e-7b62-4b95-98af-86eca4853ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d: # We will only set the weights from linear and Conv2d layers, since pooling layers do not require this\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights) # nb: this takes in a function as an argument\n",
    "\n",
    "lr = 0.9 \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr) \n",
    "loss = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fc421-0938-4066-9662-128c90b07099",
   "metadata": {},
   "source": [
    "## Scaling for Training\n",
    "### Mini-batches\n",
    "When we train our model, we often split our dataset into **mini-batches**. Kind of like if you order a huge pizza, you can't eat it all at once, the neural network will struggle to train all the data at once. Let's think of two extremes of mini-batches:\n",
    "* *Very Small:* suppose you have only one example per mini-batch, you are going to update your network after every single training example. This is very unstable for the training algorithm (it's like trying to survey one person and then using those results to represent the whole Australian population!) it's difficult to know how to update your weights to improve the model with just one example.\n",
    "* *Very Large:* suppose you have the entire dataset as your mini-batch, you are going to update your network after it evaluates every single example. This will be slow and take up a lot of memory.\n",
    "\n",
    "Just like with many things in life, there is a trade-off. We have selected a batch-size of `512` (`256` if you are using your own device) earlier on as we read in the data.\n",
    "\n",
    "### Epochs\n",
    "We also train our neural network over our dataset many times. It's like when you first meet someone, you might struggle to recall their face, but after meeting them many more times, the face becomes very familiar to you. Each time we train over our entire dataset is called an **epoch**.  \n",
    "* Too few epochs and the model does not get a chance to capture the patterns in the data.\n",
    "* Too many epochs and the model overfits and will not generalise well (we shall see an example later).\n",
    "\n",
    "The `num_epochs` variable below captures how many epochs we want to train over (set this to `10` if you are using your own device).\n",
    "\n",
    "**Warning:** The code below will take around 10-15 minutes to run (depending on the number of epochs). Feel free to grab a coffee while you wait. If you ran this on your own machine it will be *much* quicker.\n",
    "\n",
    "![](../images/train_time.png)\n",
    "\n",
    "([source](https://www.reddit.com/r/ProgrammerHumor/comments/9cu51a/shamelessly_stolen_from_xkcd_credit_where_is_due/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62085e3-cbce-41fa-9448-804099137e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 # Set this to 10 if you are using your own device\n",
    "epochs = np.arange(num_epochs) + 1\n",
    "\n",
    "def train_network_scaled(net, num_epochs):\n",
    "    timer = helper.Timer() \n",
    "    # Keep track of accuracy for each epoch\n",
    "    train_accuracy = np.array([])\n",
    "    test_accuracy = np.array([])\n",
    "\n",
    "    print(\"=== Starting Neural Network Training Now ===\")\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = helper.Accumulator(3) # define a 3d accumulator\n",
    "        net.train() # set to train\n",
    "        timer.start()\n",
    "        for i, (X, y) in enumerate(train_iter): # Loop thru each mini-batch\n",
    "            optimizer.zero_grad() # before running the forward/backward pass we need to reset the gradient (otherwise it accumulates)\n",
    "            y_hat = net(X) # Forward pass on the data to make prediction\n",
    "            l = loss(y_hat, y) # calculate the loss \n",
    "            l.backward() # back propagate the loss\n",
    "            optimizer.step() # step forward in optimisation\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], helper.accuracy(y_hat, y), X.shape[0]) # mini-batch loss,  # matches, # total examples\n",
    "            train_l = metric[0] / metric[2] # loss per unit \n",
    "            train_acc = metric[1] / metric[2] # training accuracy\n",
    "        test_acc = helper.evaluate_accuracy(net, test_iter)\n",
    "        timer.stop()\n",
    "\n",
    "        train_accuracy = np.append(train_accuracy, train_acc)\n",
    "        test_accuracy = np.append(test_accuracy, test_acc)\n",
    "        print(\"Epoch Number\", epoch, \"Trained --\", f'{timer.sum():.1f} cumulative sec taken ')\n",
    "\n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    print(f'{timer.sum():.1f} sec taken ')\n",
    "    \n",
    "    return (train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6efee-1410-4881-8012-4377ad9b9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy, test_accuracy = train_network_scaled(net = net, num_epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c0a45-9397-4ac8-be1f-9c8124431297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot it\n",
    "plt.plot(epochs, train_accuracy, label = \"Training Accuracy\")\n",
    "plt.plot(epochs, test_accuracy, label = \"Testing Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"# Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6797f4-349e-400c-addf-c234543f5306",
   "metadata": {},
   "source": [
    "With around 5 epochs, you can see that our performance on *both* train and test set have improved considerably from ~10% to around 40-50%. These are still not great, but we have now reached the limits of binder.\n",
    "\n",
    "If you are using your own machine, I highly encourage you to try 25-30 epochs. We should be able to reach accuracies of almost 80-90% with sufficient training on our LeNet. For instructions on how to set up on your own machine, refer to the description in the main [github page](https://github.com/bbpi2/cnn-pytorch-tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce92ad-8620-4f7c-87b1-53288d4ff43a",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "How did that compare with our previous model? How does that compare with random assignment? What can be done to improve the model?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e032144-deca-4dd5-82ae-4b79bc2472ff",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "We have talked a lot about how overfitting is bad, but how do we actually tell when a model has overfitted? Recall that:\n",
    "> Overfitting = performs well on data it's trained on but cannot generalise to unseen data (eg. test)\n",
    "\n",
    "So if your train accuracy improves but the test accuracy plateaus, then you know you have overfitted! See the example below for a model trained on 500 epochs:\n",
    "\n",
    "![overfit](../images/overfit.png)\n",
    "\n",
    "***Note:*** In practice we will assess the optimal number of epochs on the validate set *NOT* the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8eeca6-adc7-4d82-a757-ec6157919b3e",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "What is the optimal number of epochs from the graph above?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84558422-fcca-4cdc-9086-218ea687b7d7",
   "metadata": {},
   "source": [
    "## Compare with MLP\n",
    "Everything we have done so far is predicated on the assumption that Convolutional Neural Networks actually perform better than the simple Multi-Layer Perceptron (MLP). Why not give that hypothesis a test. We have run this exact training process on an MLP with a single 15-node hidden layer. Below are the accuracy plots for 5 epochs and 250 epochs:\n",
    "\n",
    "**5 Epochs**  \n",
    "![](../images/mlp_5.png)\n",
    "\n",
    "**250 Epochs**  \n",
    "![](../images/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e18f48-dc8d-49c4-884f-6c086c24a1ea",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "In what way is CNN preferred over MLP? Why might MLP look better in the 5 epoch plot?\n",
    "\n",
    "<font color=Red>  As a bonus question, try implementing a 15 node MLP with the sigmoid as the activation function: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93bfa4-b44f-44b0-8c74-1b210a717a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb056d8a-bbb5-4d9a-aee2-5355e33c9dc1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bf112-b14b-49c7-b931-977f6c5e3841",
   "metadata": {},
   "source": [
    "# üìì Appendix\n",
    "\n",
    "## <font color='red'> A Lesson on Data Structures: `DataLoader` vs. `DataSet` </font>\n",
    "PyTorch natively provides two data structures to work with. `DataLoader` and `DataSet`. Here's a bit of comparison:\n",
    "\n",
    "| `DataSet` | `DataLoader` |\n",
    "| --- | --- |\n",
    "| Typical dataset object (like a table) | An iterator object |\n",
    "| Reads in all the data at once and stores in memory | Reads in data only when the function is called |\n",
    "| Good for smaller datasets | Good for larger datasets |\n",
    "\n",
    "When working with big data, it becomes essential to use an iterator like the `DataLoader` object, since we rarely have enough memory to store the 10-20GB of data (not do we need to). Refer to [the PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209125d-9aaf-4766-81c4-bb6e5a3d1ed2",
   "metadata": {},
   "source": [
    "## References\n",
    "If you would like to know more:\n",
    "* Stanford University does this [excellent course](https://cs231n.github.io/) on CNNs\n",
    "* A must-read (and free!) [textbook](http://d2l.ai/) on deep learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-tutorial",
   "language": "python",
   "name": "cnn-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
